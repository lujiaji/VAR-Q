{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0319b9fd",
   "metadata": {},
   "source": [
    "### This notebook displays the basic inference of Infinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2475caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "torch.cuda.set_device(0)\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "project_root = 'VAR-Q'\n",
    "os.chdir(project_root)\n",
    "sys.path.append('VAR-Q')\n",
    "\n",
    "from Infinity.tools.run_infinity import *\n",
    "from Infinity.tools.run_infinity import _import_dynamic_resolution\n",
    "############# Configuration File #############\n",
    "# Path to the configuration file\n",
    "CONFIG_FILE = \"VAR-Q/VAR_Q/Infinity-VAR_Q-8.json\"\n",
    "\n",
    "# Load configuration from file\n",
    "print(f\"[Config] Loading configuration from {CONFIG_FILE}\")\n",
    "\n",
    "# Add VAR_Q to path for config loading\n",
    "var_q_path = 'VAR-Q/VAR_Q'\n",
    "if os.path.exists(var_q_path):\n",
    "    sys.path.append(var_q_path)\n",
    "    from config_loader import VARQConfig\n",
    "    \n",
    "    try:\n",
    "        config = VARQConfig(CONFIG_FILE)\n",
    "        \n",
    "        # Get all configuration sections\n",
    "        model_config = config.get_model_config()\n",
    "        quant_config = config.get_quantization_config()\n",
    "        inference_config = config.get_inference_config()\n",
    "        batch_config = config.get_batch_processing_config()\n",
    "        checkpoint_config = config.get_checkpoint_config()\n",
    "        \n",
    "        print(f\"[Config] Configuration loaded successfully!\")\n",
    "        print(f\"[Config] Model: {model_config.get('model_type')}\")\n",
    "        print(f\"[Config] VAR-Q Quantization: {'enabled' if quant_config.get('enable') else 'disabled'}\")\n",
    "        if quant_config.get('enable'):\n",
    "            print(f\"[Config]   - q_bits: {quant_config.get('q_bits')}\")\n",
    "            print(f\"[Config]   - quant_method: {quant_config.get('quant_method')}\")\n",
    "            print(f\"[Config]   - qkv_format: {quant_config.get('qkv_format')}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to load configuration: {e}\")\n",
    "        raise\n",
    "else:\n",
    "    print(f\"[Error] VAR_Q path not found: {var_q_path}\")\n",
    "    raise FileNotFoundError(f\"VAR_Q directory not found at {var_q_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bbf856",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Create args from configuration #############\n",
    "# Determine model-specific parameters based on model type\n",
    "model_type = model_config.get('model_type', 'infinity_2b')\n",
    "\n",
    "if model_type == \"infinity_2b\":\n",
    "    vae_type = 32\n",
    "    apply_spatial_patchify = 0\n",
    "    checkpoint_type = \"torch\"\n",
    "elif model_type == \"infinity_8b\":\n",
    "    vae_type = 14\n",
    "    apply_spatial_patchify = 1\n",
    "    checkpoint_type = \"torch_shard\"\n",
    "else:\n",
    "    # Default to 2b configuration\n",
    "    vae_type = 32\n",
    "    apply_spatial_patchify = 0\n",
    "    checkpoint_type = \"torch\"\n",
    "\n",
    "# Create args object from configuration\n",
    "args = argparse.Namespace(\n",
    "    # Model configuration\n",
    "    model_type=model_type,\n",
    "    pn='1M',  # Default to 1M, can be overridden in config if needed\n",
    "    \n",
    "    # Checkpoint paths from config\n",
    "    model_path=checkpoint_config.get('model_path'),\n",
    "    vae_path=checkpoint_config.get('vae_ckpt'),\n",
    "    text_encoder_ckpt='YOUR_PATH/flan-t5-xl',  # Update this path as needed\n",
    "    \n",
    "    # Model architecture\n",
    "    vae_type=vae_type,\n",
    "    apply_spatial_patchify=apply_spatial_patchify,\n",
    "    checkpoint_type=checkpoint_type,\n",
    "    \n",
    "    # Model behavior\n",
    "    add_lvl_embeding_only_first_block=1,\n",
    "    use_bit_label=1,\n",
    "    rope2d_each_sa_layer=1,\n",
    "    rope2d_normalized_by_hw=2,\n",
    "    use_scale_schedule_embedding=0,\n",
    "    sampling_per_bits=1,\n",
    "    text_channels=2048,\n",
    "    h_div_w_template=inference_config.get('h_div_w', 1.0),\n",
    "    use_flex_attn=0,\n",
    "    \n",
    "    # System settings\n",
    "    cache_dir='/dev/shm',\n",
    "    seed=inference_config.get('seed', 0),\n",
    "    bf16=1,\n",
    "    save_file='tmp.jpg',\n",
    "    enable_model_cache=0,  # Disable model caching by default\n",
    "    \n",
    "    # Additional required parameters\n",
    "    cfg_insertion_layer=0,\n",
    "    enable_positive_prompt=0,\n",
    "    cfg=inference_config.get('cfg', 3.0),\n",
    "    tau=inference_config.get('tau', 0.5),\n",
    "    \n",
    "    # VAR-Q quantization parameters from config\n",
    "    enable_quantization=int(quant_config.get('enable', False)),\n",
    "    q_bits=quant_config.get('q_bits', 8),\n",
    "    quant_method=quant_config.get('quant_method', 'G_SCALE_HEAD_DIM'),\n",
    "    qkv_format=quant_config.get('qkv_format', 'BLHc'),\n",
    ")\n",
    "\n",
    "print(f\"[Args] Arguments created from configuration:\")\n",
    "print(f\"  - Model: {args.model_type}\")\n",
    "print(f\"  - Model path: {args.model_path}\")\n",
    "print(f\"  - VAE path: {args.vae_path}\")\n",
    "print(f\"  - VAE type: {args.vae_type}\")\n",
    "print(f\"  - VAR-Q Quantization: {'Enabled' if args.enable_quantization else 'Disabled'}\")\n",
    "if args.enable_quantization:\n",
    "    print(f\"    * Bits: {args.q_bits}\")\n",
    "    print(f\"    * Method: {args.quant_method}\")\n",
    "    print(f\"    * Format: {args.qkv_format}\")\n",
    "print(f\"  - CFG: {inference_config.get('cfg', 3.0)}\")\n",
    "print(f\"  - Tau: {inference_config.get('tau', 0.5)}\")\n",
    "print(f\"  - Seed: {args.seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bd5cc9",
   "metadata": {},
   "source": [
    "### Configuration-Based Setup\n",
    "\n",
    "This notebook automatically loads all parameters from the configuration file `/Infinity-VAR_Q-8.json`.\n",
    "\n",
    "**Configuration sections:**\n",
    "- **Model**: Model type and architecture settings\n",
    "- **Quantization**: VAR-Q quantization parameters (enable/disable, bits, method, format)\n",
    "- **Inference**: CFG, tau, seed, and other inference parameters\n",
    "- **Checkpoints**: Paths to model and VAE checkpoints\n",
    "- **Batch Processing**: Batch size and iteration settings\n",
    "\n",
    "**To use a different configuration:**\n",
    "1. Update the `CONFIG_FILE` path in the first cell\n",
    "2. Ensure the configuration file follows the same JSON structure\n",
    "3. Re-run the first cell to load the new configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea4322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# load model #############\n",
    "# load text encoder\n",
    "text_tokenizer, text_encoder = load_tokenizer(t5_path=args.text_encoder_ckpt)\n",
    "# load vae\n",
    "vae = load_visual_tokenizer(args)\n",
    "# load infinity\n",
    "infinity = load_transformer(vae, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7a7af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# provide prompt and set args #############\n",
    "prompt = \"\"\"alien spaceship enterprise\"\"\" #<==set a prompt\n",
    "\n",
    "# Use inference parameters from configuration\n",
    "cfg = inference_config.get('cfg', 3.0)\n",
    "tau = inference_config.get('tau', 0.5)\n",
    "h_div_w = inference_config.get('h_div_w', 1.0)  # aspect ratio, height:width\n",
    "seed = inference_config.get('seed', 0)  # Use fixed seed from config, or set to random.randint(0, 10000)\n",
    "enable_positive_prompt = inference_config.get('enable_positivee_prompt', 0)  # Note: config has typo 'enable_positivee_prompt'\n",
    "\n",
    "print(f\"[Inference] Using parameters from configuration:\")\n",
    "print(f\"  - Prompt: {prompt}\")\n",
    "print(f\"  - CFG: {cfg}\")\n",
    "print(f\"  - Tau: {tau}\")\n",
    "print(f\"  - H/W ratio: {h_div_w}\")\n",
    "print(f\"  - Seed: {seed}\")\n",
    "print(f\"  - Enable positive prompt: {enable_positive_prompt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9296aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "############# generate image #############\n",
    "# Ensure dynamic resolution is loaded\n",
    "if 'dynamic_resolution_h_w' not in globals() or dynamic_resolution_h_w is None:\n",
    "    print(\"[Inference] Loading dynamic resolution...\")\n",
    "    dynamic_resolution_h_w, h_div_w_templates = _import_dynamic_resolution()\n",
    "    print(\"[Inference] Dynamic resolution loaded successfully!\")\n",
    "\n",
    "h_div_w_template_ = h_div_w_templates[np.argmin(np.abs(h_div_w_templates-h_div_w))]\n",
    "scale_schedule = dynamic_resolution_h_w[h_div_w_template_][args.pn]['scales']\n",
    "scale_schedule = [(1, h, w) for (_, h, w) in scale_schedule]\n",
    "generated_image = gen_one_img(\n",
    "    infinity,\n",
    "    vae,\n",
    "    text_tokenizer,\n",
    "    text_encoder,\n",
    "    prompt,\n",
    "    g_seed=seed,\n",
    "    gt_leak=0,\n",
    "    gt_ls_Bl=None,\n",
    "    cfg_list=cfg,\n",
    "    tau_list=tau,\n",
    "    scale_schedule=scale_schedule,\n",
    "    cfg_insertion_layer=[args.cfg_insertion_layer],\n",
    "    vae_type=args.vae_type,\n",
    "    sampling_per_bits=args.sampling_per_bits,\n",
    "    enable_positive_prompt=enable_positive_prompt,\n",
    ")\n",
    "args.save_file = 'Benchmark/outputs/Infinity/ipynb_tmp.jpg'\n",
    "os.makedirs(osp.dirname(osp.abspath(args.save_file)), exist_ok=True)\n",
    "cv2.imwrite(args.save_file, generated_image.cpu().numpy())\n",
    "print(f'Save to {osp.abspath(args.save_file)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
